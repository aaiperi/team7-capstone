{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0x-f8pX_6y8"
   },
   "source": [
    "# Semantic Segmentation vs Object Detection 트래킹 비교 (개선된 성능 측정)\n",
    "\n",
    "이 노트북은 원본 GridClustering 구조를 유지하면서 Semantic Segmentation 기반 트래킹과 순정 YOLO 기반 Object Detection의 성능을 비교합니다.\n",
    "두 방식을 별도로 실행하여 메모리 사용량과 성능을 정확하게 비교할 수 있도록 수정되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXrR1yLP_6y9"
   },
   "source": [
    "## 필요한 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsFGP3gS_6y9"
   },
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install opencv-python numpy torch matplotlib tqdm scikit-image psutil transformers pandas ultralytics"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs_7zUgm_6y9"
   },
   "source": [
    "## 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ey91Aqn_6y9"
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.measure import regionprops, label\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "import math\n",
    "import pandas as pd\n",
    "from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 랜덤 시드 설정\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs_7zUgm_6y9"
   },
   "source": [
    "## 8방위 방향 계산 함수\n",
    "\n",
    "객체의 이동 방향을 8방위(N, NE, E, SE, S, SW, W, NW)로 계산하는 함수들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ey91Aqn_6y9"
   },
   "source": [
    "def calculate_8_direction(dx, dy):\n",
    "    \"\"\"\n",
    "    Calculate the direction based on 8 cardinal directions:\n",
    "    N, NE, E, SE, S, SW, W, NW\n",
    "\n",
    "    Args:\n",
    "        dx: Change in x-coordinate\n",
    "        dy: Change in y-coordinate (note: in image coordinates, y increases downward)\n",
    "\n",
    "    Returns:\n",
    "        String representing the direction\n",
    "    \"\"\"\n",
    "    # Calculate angle in radians and convert to degrees\n",
    "    angle = math.atan2(dy, dx) * 180 / math.pi\n",
    "\n",
    "    # Adjust angle to be between 0 and 360 degrees\n",
    "    if angle < 0:\n",
    "        angle += 360\n",
    "\n",
    "    # Define direction based on angle\n",
    "    if 22.5 <= angle < 67.5:\n",
    "        return \"SE\"  # South-East\n",
    "    elif 67.5 <= angle < 112.5:\n",
    "        return \"S\"   # South\n",
    "    elif 112.5 <= angle < 157.5:\n",
    "        return \"SW\"  # South-West\n",
    "    elif 157.5 <= angle < 202.5:\n",
    "        return \"W\"   # West\n",
    "    elif 202.5 <= angle < 247.5:\n",
    "        return \"NW\"  # North-West\n",
    "    elif 247.5 <= angle < 292.5:\n",
    "        return \"N\"   # North\n",
    "    elif 292.5 <= angle < 337.5:\n",
    "        return \"NE\"  # North-East\n",
    "    else:  # angle < 22.5 or angle >= 337.5\n",
    "        return \"E\"   # East\n",
    "\n",
    "def calculate_direction_accuracy(directions):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of direction tracking\n",
    "\n",
    "    Args:\n",
    "        directions: List of direction strings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with dominant direction and accuracy percentage\n",
    "    \"\"\"\n",
    "    if not directions:\n",
    "        return {\n",
    "            'dominant_direction': 'N/A',\n",
    "            'direction_accuracy': 0.0\n",
    "        }\n",
    "\n",
    "    # Count occurrences of each direction\n",
    "    direction_counts = {}\n",
    "    for direction in directions:\n",
    "        if direction in direction_counts:\n",
    "            direction_counts[direction] += 1\n",
    "        else:\n",
    "            direction_counts[direction] = 1\n",
    "\n",
    "    # Find the most common direction\n",
    "    dominant_direction = max(direction_counts, key=direction_counts.get)\n",
    "    correct_count = direction_counts[dominant_direction]\n",
    "    accuracy = (correct_count / len(directions)) * 100\n",
    "\n",
    "    return {\n",
    "        'dominant_direction': dominant_direction,\n",
    "        'direction_accuracy': round(accuracy, 2)\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs_7zUgm_6y9"
   },
   "source": [
    "## GridClustering 원본 구현\n",
    "\n",
    "원본 GridClustering 구현을 최대한 유지하면서 8방위 방향 시스템을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ey91Aqn_6y9"
   },
   "source": [
    "class GridClustering:\n",
    "    def __init__(self, grid_size=50, min_samples=3):\n",
    "        self.grid_size = grid_size\n",
    "        self.min_samples = min_samples\n",
    "        self.tracks = []\n",
    "        self.next_id = 0\n",
    "        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "        self.roi_mask = None\n",
    "        self.direction_data = {}  # 트랙별 방향 데이터 저장\n",
    "\n",
    "    def set_roi(self, roi_mask):\n",
    "        \"\"\"ROI 마스크 설정 (처음에 한 번만 설정)\"\"\"\n",
    "        self.roi_mask = roi_mask\n",
    "\n",
    "    def detect_moving_objects(self, frame):\n",
    "        \"\"\"배경 차분을 사용하여 움직이는 객체 검출\"\"\"\n",
    "        # 배경 차분 적용\n",
    "        fg_mask = self.bg_subtractor.apply(frame)\n",
    "        fg_mask[fg_mask == 127] = 0  # 그림자 제거\n",
    "\n",
    "        # 그림자 및 헤드라이트 감지\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        shadow_mask = (v < 0.3 * 255).astype(np.uint8) * 255\n",
    "        highlight_mask = ((v > 220) & (s < 50)).astype(np.uint8) * 255\n",
    "\n",
    "        fg_mask[shadow_mask > 0] = fg_mask[shadow_mask > 0] * 0.7\n",
    "        fg_mask[highlight_mask > 0] = fg_mask[highlight_mask > 0] * 0.6\n",
    "\n",
    "        # ROI 마스크가 있으면 적용\n",
    "        if self.roi_mask is not None:\n",
    "            moving_objects_mask = np.zeros_like(fg_mask)\n",
    "            moving_objects_mask[self.roi_mask > 0] = fg_mask[self.roi_mask > 0]\n",
    "        else:\n",
    "            moving_objects_mask = fg_mask\n",
    "\n",
    "        # 모폴로지 연산으로 노이즈 제거\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        moving_objects_mask = cv2.morphologyEx(moving_objects_mask, cv2.MORPH_OPEN, kernel)\n",
    "        moving_objects_mask = cv2.morphologyEx(moving_objects_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # 레이블링\n",
    "        labeled_mask, num_labels = label(moving_objects_mask > 0, return_num=True, connectivity=2)\n",
    "\n",
    "        return labeled_mask, fg_mask\n",
    "\n",
    "    def extract_clusters(self, frame, labeled_mask, min_area=100):\n",
    "        \"\"\"레이블링된 마스크에서 클러스터 추출\"\"\"\n",
    "        regions = regionprops(labeled_mask)\n",
    "        clusters = []\n",
    "\n",
    "        for region in regions:\n",
    "            if region.area < min_area:  # 작은 영역 필터링\n",
    "                continue\n",
    "\n",
    "            # 바운딩 박스 계산\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            x1, y1, x2, y2 = minc, minr, maxc, maxr\n",
    "\n",
    "            # 중심점 계산\n",
    "            cy, cx = region.centroid\n",
    "\n",
    "            clusters.append({\n",
    "                'bbox': [x1, y1, x2, y2],\n",
    "                'centroid': [cx, cy],\n",
    "                'area': region.area\n",
    "            })\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def match_clusters(self, prev_clusters, curr_clusters, max_dist=50, area_similarity_threshold=0.5):\n",
    "        \"\"\"이전 프레임과 현재 프레임의 클러스터 매칭\"\"\"\n",
    "        matches = []\n",
    "        for curr in curr_clusters:\n",
    "            curr_cx, curr_cy = curr['centroid']\n",
    "            curr_area = curr['area']\n",
    "            min_dist = float('inf')\n",
    "            best_match = None\n",
    "            for prev in prev_clusters:\n",
    "                prev_cx, prev_cy = prev['centroid']\n",
    "                prev_area = prev['area']\n",
    "                area_ratio = min(prev_area, curr_area) / max(prev_area, curr_area)\n",
    "                if area_ratio < area_similarity_threshold:\n",
    "                    continue\n",
    "                dist = np.sqrt((curr_cx - prev_cx)**2 + (curr_cy - prev_cy)**2)\n",
    "                if dist < min_dist and dist < max_dist:\n",
    "                    min_dist = dist\n",
    "                    best_match = prev\n",
    "            matches.append((best_match, curr))\n",
    "        return matches\n",
    "\n",
    "    def update_tracks(self, matches, frame_count, max_trace_length=50, trace_duration=30, stationary_threshold=3):\n",
    "        \"\"\"트랙 업데이트\"\"\"\n",
    "        # 첫 프레임이면 모든 클러스터를 새 트랙으로 초기화\n",
    "        if not self.tracks:\n",
    "            for _, curr in matches:\n",
    "                self.tracks.append([{\n",
    "                    'id': self.next_id,\n",
    "                    'centroid': curr['centroid'],\n",
    "                    'bbox': curr['bbox'],\n",
    "                    'area': curr['area'],\n",
    "                    'frame': frame_count,\n",
    "                    'stationary_count': 0\n",
    "                }])\n",
    "                self.next_id += 1\n",
    "            return\n",
    "\n",
    "        # 매칭 결과에 따라 트랙 업데이트\n",
    "        updated_track_ids = set()\n",
    "        for prev, curr in matches:\n",
    "            if prev is None:  # 새 클러스터\n",
    "                self.tracks.append([{\n",
    "                    'id': self.next_id,\n",
    "                    'centroid': curr['centroid'],\n",
    "                    'bbox': curr['bbox'],\n",
    "                    'area': curr['area'],\n",
    "                    'frame': frame_count,\n",
    "                    'stationary_count': 0\n",
    "                }])\n",
    "                self.next_id += 1\n",
    "            else:  # 기존 트랙 업데이트\n",
    "                track_id = prev['id']\n",
    "                track_idx = None\n",
    "                for i, track in enumerate(self.tracks):\n",
    "                    if track and track[-1]['id'] == track_id:\n",
    "                        track_idx = i\n",
    "                        break\n",
    "\n",
    "                if track_idx is not None:\n",
    "                    # 정지 상태 확인\n",
    "                    prev_cx, prev_cy = prev['centroid']\n",
    "                    curr_cx, curr_cy = curr['centroid']\n",
    "                    dist = np.sqrt((curr_cx - prev_cx)**2 + (curr_cy - prev_cy)**2)\n",
    "\n",
    "                    stationary_count = prev['stationary_count']\n",
    "                    if dist < stationary_threshold:\n",
    "                        stationary_count += 1\n",
    "                    else:\n",
    "                        stationary_count = 0\n",
    "\n",
    "                    # 트랙 업데이트\n",
    "                    self.tracks[track_idx].append({\n",
    "                        'id': track_id,\n",
    "                        'centroid': curr['centroid'],\n",
    "                        'bbox': curr['bbox'],\n",
    "                        'area': curr['area'],\n",
    "                        'frame': frame_count,\n",
    "                        'stationary_count': stationary_count\n",
    "                    })\n",
    "\n",
    "                    # 트랙 길이 제한\n",
    "                    if len(self.tracks[track_idx]) > max_trace_length:\n",
    "                        self.tracks[track_idx] = self.tracks[track_idx][-max_trace_length:]\n",
    "\n",
    "                    # 방향 데이터 업데이트 (8방위)\n",
    "                    if len(self.tracks[track_idx]) >= 2:\n",
    "                        dx = curr['centroid'][0] - prev['centroid'][0]\n",
    "                        dy = curr['centroid'][1] - prev['centroid'][1]\n",
    "                        direction = calculate_8_direction(dx, dy)\n",
    "\n",
    "                        if track_id not in self.direction_data:\n",
    "                            self.direction_data[track_id] = []\n",
    "                        self.direction_data[track_id].append(direction)\n",
    "\n",
    "                    updated_track_ids.add(track_idx)\n",
    "\n",
    "        # 오래된 트랙 제거\n",
    "        for i in range(len(self.tracks) - 1, -1, -1):\n",
    "            if i not in updated_track_ids and self.tracks[i] and frame_count - self.tracks[i][-1]['frame'] > trace_duration:\n",
    "                self.tracks[i] = None\n",
    "\n",
    "        # None 값 제거\n",
    "        self.tracks = [track for track in self.tracks if track is not None]\n",
    "\n",
    "    def cluster_tracks_by_grid(self):\n",
    "        \"\"\"트랙을 그리드 기반으로 클러스터링\"\"\"\n",
    "        if not self.tracks:\n",
    "            return [], []\n",
    "\n",
    "        # 그리드 기반 클러스터링을 위한 딕셔너리\n",
    "        grid_to_tracks = {}\n",
    "        track_indices = []\n",
    "\n",
    "        for idx, track in enumerate(self.tracks):\n",
    "            if len(track) < 2:\n",
    "                continue\n",
    "\n",
    "            # 트랙의 시작 위치 (첫 번째 클러스터의 중심점)\n",
    "            start_x, start_y = track[0]['centroid']\n",
    "\n",
    "            # 그리드 셀 계산\n",
    "            grid_x = int(start_x // self.grid_size)\n",
    "            grid_y = int(start_y // self.grid_size)\n",
    "            grid_key = (grid_x, grid_y)\n",
    "\n",
    "            # 그리드 셀에 트랙 추가\n",
    "            if grid_key not in grid_to_tracks:\n",
    "                grid_to_tracks[grid_key] = []\n",
    "            grid_to_tracks[grid_key].append(idx)\n",
    "            track_indices.append(idx)\n",
    "\n",
    "        # 클러스터 레이블 생성\n",
    "        labels = [-1] * len(self.tracks)\n",
    "        current_label = 0\n",
    "        valid_tracks = []\n",
    "\n",
    "        for grid_key, track_ids in grid_to_tracks.items():\n",
    "            if len(track_ids) >= self.min_samples:\n",
    "                for idx in track_ids:\n",
    "                    labels[idx] = current_label\n",
    "                valid_tracks.extend([self.tracks[idx] for idx in track_ids])\n",
    "                current_label += 1\n",
    "\n",
    "        valid_labels = [labels[idx] for idx in track_indices if labels[idx] != -1]\n",
    "        return valid_tracks, valid_labels\n",
    "\n",
    "    def get_direction_accuracy(self):\n",
    "        \"\"\"8방위 방향 정확도 계산\"\"\"\n",
    "        direction_results = {}\n",
    "\n",
    "        for track_id, directions in self.direction_data.items():\n",
    "            if len(directions) < 2:\n",
    "                continue\n",
    "\n",
    "            direction_results[track_id] = calculate_direction_accuracy(directions)\n",
    "\n",
    "        return direction_results\n",
    "\n",
    "    def process_frame(self, frame, frame_count):\n",
    "        \"\"\"프레임 처리\"\"\"\n",
    "        # 움직이는 객체 검출\n",
    "        labeled_mask, fg_mask = self.detect_moving_objects(frame)\n",
    "\n",
    "        # 클러스터 추출\n",
    "        curr_clusters = self.extract_clusters(frame, labeled_mask)\n",
    "\n",
    "        # 이전 클러스터 가져오기\n",
    "        prev_clusters = []\n",
    "        for track in self.tracks:\n",
    "            if track:\n",
    "                prev_clusters.append(track[-1])\n",
    "\n",
    "        # 클러스터 매칭\n",
    "        matches = self.match_clusters(prev_clusters, curr_clusters)\n",
    "\n",
    "        # 트랙 업데이트\n",
    "        self.update_tracks(matches, frame_count)\n",
    "\n",
    "        return labeled_mask, fg_mask, curr_clusters\n",
    "\n",
    "    def draw_tracks(self, frame, color_by_direction=True):\n",
    "        \"\"\"트랙 시각화\"\"\"\n",
    "        vis_frame = frame.copy()\n",
    "\n",
    "        # ROI 표시\n",
    "        if self.roi_mask is not None:\n",
    "            roi_contours, _ = cv2.findContours(self.roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_frame, roi_contours, -1, (0, 0, 255), 2)\n",
    "\n",
    "        # 방향별 색상 설정\n",
    "        direction_colors = {\n",
    "            \"N\": (255, 0, 0),     # 빨강\n",
    "            \"NE\": (255, 128, 0),  # 주황\n",
    "            \"E\": (255, 255, 0),   # 노랑\n",
    "            \"SE\": (128, 255, 0),  # 연두\n",
    "            \"S\": (0, 255, 0),     # 초록\n",
    "            \"SW\": (0, 255, 128),  # 청록\n",
    "            \"W\": (0, 128, 255),   # 하늘\n",
    "            \"NW\": (0, 0, 255)     # 파랑\n",
    "        }\n",
    "\n",
    "        # 트랙 그리기\n",
    "        for track in self.tracks:\n",
    "            if not track or len(track) < 2:\n",
    "                continue\n",
    "\n",
    "            track_id = track[0]['id']\n",
    "\n",
    "            # 방향 계산\n",
    "            first_centroid = track[0]['centroid']\n",
    "            last_centroid = track[-1]['centroid']\n",
    "            dx = last_centroid[0] - first_centroid[0]\n",
    "            dy = last_centroid[1] - first_centroid[1]\n",
    "            direction = calculate_8_direction(dx, dy)\n",
    "\n",
    "            # 색상 설정\n",
    "            if color_by_direction:\n",
    "                color = direction_colors.get(direction, (255, 255, 255))\n",
    "            else:\n",
    "                # 트랙 ID에 따라 색상 설정\n",
    "                color = ((track_id * 50) % 255, (track_id * 100) % 255, (track_id * 150) % 255)\n",
    "\n",
    "            # 트랙 그리기\n",
    "            points = []\n",
    "            for detection in track:\n",
    "                cx, cy = detection['centroid']\n",
    "                points.append((int(cx), int(cy)))\n",
    "\n",
    "            # 경로 그리기 (그라데이션 효과)\n",
    "            for i in range(1, len(points)):\n",
    "                alpha = 0.5 + 0.5 * (i / len(points))  # 투명도 그라데이션\n",
    "                thickness = 1 + int(2 * (i / len(points)))  # 두께 그라데이션\n",
    "                cv2.line(vis_frame, points[i-1], points[i], color, thickness)\n",
    "\n",
    "            # 현재 위치 표시\n",
    "            cx, cy = points[-1]\n",
    "            cv2.circle(vis_frame, (cx, cy), 5, color, -1)\n",
    "\n",
    "            # 바운딩 박스 그리기\n",
    "            x1, y1, x2, y2 = track[-1]['bbox']\n",
    "            cv2.rectangle(vis_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "\n",
    "            # 트랙 ID 및 방향 표시\n",
    "            text = f\"ID:{track_id} {direction}\"\n",
    "            cv2.putText(vis_frame, text, (int(cx) + 10, int(cy)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        return vis_frame"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKbK_fKD_6y_"
   },
   "source": [
    "## Semantic Segmentation 기반 트래킹 구현\n",
    "\n",
    "Semantic Segmentation 모델을 사용하여 ROI를 설정하고 객체를 추적하는 함수들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qija7L5_6y_"
   },
   "source": [
    "# 세그멘테이션 모델 설정\n",
    "def setup_segmentation_model():\n",
    "    model_name = \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n",
    "    print(f\"모델 '{model_name}' 로드 중...\")\n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    cityscapes_classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
    "                          'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
    "                          'truck', 'bus', 'train', 'motorcycle', 'bicycle']\n",
    "    road_classes = [i for i, name in enumerate(cityscapes_classes) if name in ['road']]\n",
    "    vehicle_classes = [i for i, name in enumerate(cityscapes_classes) if name in ['car', 'truck', 'bus']]\n",
    "\n",
    "    print(f\"도로 클래스: {[(i, cityscapes_classes[i]) for i in road_classes]}\")\n",
    "    print(f\"탈것 클래스: {[(i, cityscapes_classes[i]) for i in vehicle_classes]}\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    return model, image_processor, device, road_classes, vehicle_classes, cityscapes_classes\n",
    "\n",
    "# 첫 프레임에서 ROI 설정\n",
    "def get_first_frame_roi(video_path, model, image_processor, device, road_classes, vehicle_classes, stabilization_frames=15):\n",
    "    \"\"\"\n",
    "    첫 프레임에서 ROI를 설정하고 배경 모델을 안정화하는 함수\n",
    "\n",
    "    Args:\n",
    "        video_path: 비디오 파일 경로\n",
    "        model: 세그멘테이션 모델\n",
    "        image_processor: 이미지 프로세서\n",
    "        device: 디바이스\n",
    "        road_classes: 도로 클래스 인덱스\n",
    "        vehicle_classes: 차량 클래스 인덱스\n",
    "        stabilization_frames: 안정화를 위한 프레임 수\n",
    "\n",
    "    Returns:\n",
    "        ROI 마스크, 프레임 리스트\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(\"비디오 파일이 없습니다.\")\n",
    "        return None, None\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: 프레임을 읽을 수 없습니다.\")\n",
    "        cap.release()\n",
    "        return None, None\n",
    "\n",
    "    print(\"첫 프레임에서 도로와 탈것 세그멘테이션 수행 중...\")\n",
    "    roi_mask = segment_frame(model, image_processor, frame, device, road_classes, vehicle_classes)\n",
    "\n",
    "    # 추가 프레임 읽기 (안정화 목적)\n",
    "    frames = [frame]\n",
    "    for i in range(stabilization_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    print(\"첫 프레임에서 고정 ROI 설정 완료!\")\n",
    "    print(f\"ROI 영역 비율: {np.sum(roi_mask > 0) / (roi_mask.shape[0] * roi_mask.shape[1]) * 100:.2f}%\")\n",
    "\n",
    "    return roi_mask, frames\n",
    "\n",
    "# 세그멘테이션 기반 ROI 생성\n",
    "def segment_frame(model, image_processor, frame, device, road_classes, vehicle_classes):\n",
    "    \"\"\"\n",
    "    세그멘테이션 모델을 사용하여 ROI 마스크 생성\n",
    "\n",
    "    Args:\n",
    "        model: 세그멘테이션 모델\n",
    "        image_processor: 이미지 프로세서\n",
    "        frame: 입력 프레임\n",
    "        device: 디바이스\n",
    "        road_classes: 도로 클래스 인덱스\n",
    "        vehicle_classes: 차량 클래스 인덱스\n",
    "\n",
    "    Returns:\n",
    "        ROI 마스크\n",
    "    \"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    inputs = image_processor(images=rgb_frame, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    upsampled_logits = torch.nn.functional.interpolate(logits, size=rgb_frame.shape[:2], mode=\"bilinear\", align_corners=False)\n",
    "    sem_seg = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    road_mask = np.zeros_like(sem_seg, dtype=np.uint8)\n",
    "    for road_class in road_classes:\n",
    "        road_mask[sem_seg == road_class] = 255\n",
    "\n",
    "    vehicle_mask = np.zeros_like(sem_seg, dtype=np.uint8)\n",
    "    for vehicle_class in vehicle_classes:\n",
    "        vehicle_mask[sem_seg == vehicle_class] = 255\n",
    "\n",
    "    combined_roi_mask = np.zeros_like(sem_seg, dtype=np.uint8)\n",
    "    combined_roi_mask[road_mask > 0] = 255\n",
    "    combined_roi_mask[vehicle_mask > 0] = 255\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    combined_roi_mask = cv2.morphologyEx(combined_roi_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return combined_roi_mask"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnDUiSMC_6y_"
   },
   "source": [
    "## Semantic Segmentation 기반 트래킹 실행 (개선된 성능 측정)\n",
    "\n",
    "Semantic Segmentation 모델을 사용하여 트래킹을 실행하고 결과를 분석합니다.\n",
    "성능 측정 방식을 개선하여 더 정확한 비교가 가능하도록 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mE_srXiq_6y_"
   },
   "source": [
    "def run_segmentation_tracking(video_path, max_frames=100):\n",
    "    \"\"\"Semantic Segmentation 기반 트래킹 실행 (개선된 성능 측정)\"\"\"\n",
    "    # 전체 파이프라인 시간 측정 시작\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # 비디오 열기\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: 비디오 파일을 열 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 비디오 정보\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if max_frames is not None and max_frames < frame_count:\n",
    "        frame_count = max_frames\n",
    "\n",
    "    print(f\"비디오 크기: {width}x{height}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"처리할 프레임 수: {frame_count}\")\n",
    "\n",
    "    # 모델 설정 시간 측정\n",
    "    model_setup_start_time = time.time()\n",
    "    seg_model, image_processor, device, road_classes, vehicle_classes, cityscapes_classes = setup_segmentation_model()\n",
    "    model_setup_time = time.time() - model_setup_start_time\n",
    "    print(f\"Segmentation 모델 설정 시간: {model_setup_time:.4f}초\")\n",
    "\n",
    "    # 첫 프레임에서 ROI 설정 시간 측정\n",
    "    print(\"\\n첫 프레임에서 ROI 설정 중...\")\n",
    "    roi_setup_start_time = time.time()\n",
    "    roi_mask, frames = get_first_frame_roi(\n",
    "        video_path, seg_model, image_processor, device, road_classes, vehicle_classes, stabilization_frames=15\n",
    "    )\n",
    "    roi_setup_time = time.time() - roi_setup_start_time\n",
    "    print(f\"ROI 설정 시간: {roi_setup_time:.4f}초\")\n",
    "\n",
    "    # 트래커 설정\n",
    "    seg_tracker = GridClustering(grid_size=50, min_samples=3)\n",
    "    seg_tracker.set_roi(roi_mask)\n",
    "\n",
    "    # 결과 저장 변수\n",
    "    seg_processing_times = []  # 프레임 처리 시간만 측정\n",
    "    seg_detection_counts = []\n",
    "    seg_memory_usages = []\n",
    "    seg_cluster_counts = []  # 클러스터 수 추가\n",
    "\n",
    "    # 결과 영상 저장\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    seg_output = cv2.VideoWriter('segmentation_tracking.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "    # 비디오 위치 설정 (안정화 프레임 이후부터 처리)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, len(frames))\n",
    "\n",
    "    # 프레임 처리 시간 측정 시작\n",
    "    frames_processing_start_time = time.time()\n",
    "\n",
    "    # 프레임 처리\n",
    "    processed_frames = 0\n",
    "    for frame_idx in tqdm(range(min(frame_count - len(frames), frame_count))):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        processed_frames += 1\n",
    "\n",
    "        # 프레임별 처리 시간 측정\n",
    "        frame_start_time = time.time()\n",
    "        labeled_mask, fg_mask, curr_clusters = seg_tracker.process_frame(frame, frame_idx)\n",
    "        frame_time = time.time() - frame_start_time\n",
    "\n",
    "        # 시각화 시간은 측정에서 제외\n",
    "        seg_vis_frame = seg_tracker.draw_tracks(frame)\n",
    "\n",
    "        # 메모리 사용량\n",
    "        memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "        # 결과 저장\n",
    "        seg_processing_times.append(frame_time)\n",
    "        seg_detection_counts.append(len([track for track in seg_tracker.tracks if track]))\n",
    "        seg_memory_usages.append(memory_usage)\n",
    "        seg_cluster_counts.append(len(curr_clusters))\n",
    "\n",
    "        # 성능 정보 표시\n",
    "        seg_fps_text = f\"Seg FPS: {1/frame_time:.1f}\"\n",
    "        seg_count_text = f\"Seg Tracks: {len([track for track in seg_tracker.tracks if track])}\"\n",
    "        seg_memory_text = f\"Memory: {memory_usage:.1f} MB\"\n",
    "        seg_clusters_text = f\"Clusters: {len(curr_clusters)}\"\n",
    "\n",
    "        cv2.putText(seg_vis_frame, seg_fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(seg_vis_frame, seg_count_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(seg_vis_frame, seg_memory_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(seg_vis_frame, seg_clusters_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # 결과 영상 저장\n",
    "        seg_output.write(seg_vis_frame)\n",
    "\n",
    "    # 프레임 처리 시간 측정 종료\n",
    "    frames_processing_time = time.time() - frames_processing_start_time\n",
    "\n",
    "    # 전체 파이프라인 시간 측정 종료\n",
    "    total_time = time.time() - total_start_time\n",
    "\n",
    "    # 자원 해제\n",
    "    cap.release()\n",
    "    seg_output.release()\n",
    "\n",
    "    # 방향 정확도 계산\n",
    "    seg_direction_accuracy = seg_tracker.get_direction_accuracy()\n",
    "\n",
    "    # 그리드 클러스터링 결과\n",
    "    seg_valid_tracks, seg_labels = seg_tracker.cluster_tracks_by_grid()\n",
    "\n",
    "    # 결과 요약\n",
    "    results = {\n",
    "        # 프레임 처리 시간만 측정한 결과\n",
    "        'avg_frame_processing_time': np.mean(seg_processing_times),\n",
    "        'avg_frame_fps': 1 / np.mean(seg_processing_times),\n",
    "\n",
    "        # 전체 파이프라인 시간 측정 결과\n",
    "        'total_pipeline_time': total_time,\n",
    "        'total_pipeline_fps': processed_frames / total_time,\n",
    "\n",
    "        # 단계별 시간 측정 결과\n",
    "        'model_setup_time': model_setup_time,\n",
    "        'roi_setup_time': roi_setup_time,\n",
    "        'frames_processing_time': frames_processing_time,\n",
    "\n",
    "        # 기타 측정 결과\n",
    "        'avg_detections': np.mean(seg_detection_counts),\n",
    "        'avg_clusters': np.mean(seg_cluster_counts),\n",
    "        'avg_memory_usage': np.mean(seg_memory_usages),\n",
    "        'direction_accuracy': seg_direction_accuracy,\n",
    "        'valid_tracks_count': len(seg_valid_tracks),\n",
    "        'cluster_count': len(set(seg_labels)) if seg_labels else 0,\n",
    "        'processed_frames': processed_frames\n",
    "    }\n",
    "\n",
    "    # 메모리 정리\n",
    "    del seg_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePLYUMVl_6zA"
   },
   "source": [
    "## Object Detection (YOLO) 기반 트래킹 구현\n",
    "\n",
    "순정 YOLOv8 모델을 사용하여 객체를 추적하는 함수들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePLYUMVl_6zA"
   },
   "source": [
    "# YOLO 모델 설정\n",
    "def setup_yolo_model():\n",
    "    model = YOLO(\"yolov8x.pt\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    print(f\"YOLO 모델 디바이스: {model.device}\")\n",
    "    return model\n",
    "\n",
    "# YOLO 기반 객체 검출\n",
    "def detect_objects_yolo(model, frame, vehicle_classes=['car', 'truck', 'bus', 'motorcycle', 'bicycle']):\n",
    "    \"\"\"\n",
    "    순정 YOLO 모델을 사용하여 객체 검출\n",
    "\n",
    "    Args:\n",
    "        model: YOLO 모델\n",
    "        frame: 입력 프레임\n",
    "        vehicle_classes: 차량 클래스 이름 목록\n",
    "\n",
    "    Returns:\n",
    "        검출된 객체 리스트, 처리 시간\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # YOLO 모델로 객체 검출\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # 클래스 이름 가져오기\n",
    "            cls = int(box.cls.item())\n",
    "            class_name = model.names[cls]\n",
    "\n",
    "            # 차량 클래스만 필터링\n",
    "            if class_name not in vehicle_classes:\n",
    "                continue\n",
    "\n",
    "            # 바운딩 박스 좌표\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "            # 중심점 계산\n",
    "            cx = (x1 + x2) / 2\n",
    "            cy = (y1 + y2) / 2\n",
    "\n",
    "            # 신뢰도\n",
    "            confidence = box.conf.item()\n",
    "\n",
    "            # 면적 계산\n",
    "            area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "            detections.append({\n",
    "                'bbox': [x1, y1, x2, y2],\n",
    "                'centroid': [cx, cy],\n",
    "                'class': class_name,\n",
    "                'confidence': confidence,\n",
    "                'area': area\n",
    "            })\n",
    "\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    return detections, processing_time"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX9CvUMO_6zA"
   },
   "source": [
    "## Object Detection (YOLO) 기반 트래킹 실행 (개선된 성능 측정)\n",
    "\n",
    "YOLO 모델을 사용하여 트래킹을 실행하고 결과를 분석합니다.\n",
    "성능 측정 방식을 개선하여 더 정확한 비교가 가능하도록 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abZMxIaJ_6zA"
   },
   "source": [
    "def run_yolo_tracking(video_path, max_frames=100):\n",
    "    \"\"\"YOLO 기반 트래킹 실행 (개선된 성능 측정)\"\"\"\n",
    "    # 전체 파이프라인 시간 측정 시작\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # 비디오 열기\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: 비디오 파일을 열 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 비디오 정보\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # max_frames 처리 개선 - None 값 처리\n",
    "    target_frames = frame_count\n",
    "    if max_frames is not None and max_frames != 'none' and max_frames < frame_count:\n",
    "        target_frames = max_frames\n",
    "\n",
    "    print(f\"비디오 크기: {width}x{height}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"총 프레임 수: {frame_count}\")\n",
    "    print(f\"처리할 프레임 수: {target_frames}\")\n",
    "\n",
    "    # 모델 설정 시간 측정\n",
    "    model_setup_start_time = time.time()\n",
    "    yolo_model = setup_yolo_model()\n",
    "    model_setup_time = time.time() - model_setup_start_time\n",
    "    print(f\"YOLO 모델 설정 시간: {model_setup_time:.4f}초\")\n",
    "\n",
    "    # 트래커 설정\n",
    "    yolo_tracker = GridClustering(grid_size=50, min_samples=3)\n",
    "    # YOLO 트래커는 ROI 없이 순정 상태로 사용\n",
    "\n",
    "    # 결과 저장 변수\n",
    "    yolo_detection_times = []  # 객체 검출 시간\n",
    "    yolo_tracking_times = []   # 트래킹 시간\n",
    "    yolo_processing_times = [] # 전체 처리 시간\n",
    "    yolo_detection_counts = []\n",
    "    yolo_memory_usages = []\n",
    "    yolo_object_counts = []    # 검출된 객체 수\n",
    "\n",
    "    # 결과 영상 저장\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    yolo_output = cv2.VideoWriter('yolo_tracking.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "    # 프레임 처리 시간 측정 시작\n",
    "    frames_processing_start_time = time.time()\n",
    "\n",
    "    # 프레임 처리\n",
    "    processed_frames = 0\n",
    "    for frame_idx in tqdm(range(target_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        processed_frames += 1\n",
    "\n",
    "        # 프레임별 처리 시간 측정 시작\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        # YOLO 객체 검출 시간 측정\n",
    "        yolo_detections, detection_time = detect_objects_yolo(yolo_model, frame)\n",
    "        yolo_detection_times.append(detection_time)\n",
    "\n",
    "        # 트래킹 시간 측정\n",
    "        tracking_start_time = time.time()\n",
    "        matches = yolo_tracker.match_clusters(\n",
    "            [track[-1] for track in yolo_tracker.tracks if track],\n",
    "            yolo_detections\n",
    "        )\n",
    "        yolo_tracker.update_tracks(matches, frame_idx)\n",
    "        tracking_time = time.time() - tracking_start_time\n",
    "        yolo_tracking_times.append(tracking_time)\n",
    "\n",
    "        # 전체 프레임 처리 시간\n",
    "        frame_time = time.time() - frame_start_time\n",
    "        yolo_processing_times.append(frame_time)\n",
    "\n",
    "        # 시각화 시간은 측정에서 제외\n",
    "        yolo_vis_frame = yolo_tracker.draw_tracks(frame)\n",
    "\n",
    "        # 메모리 사용량\n",
    "        memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "        # 결과 저장\n",
    "        yolo_detection_counts.append(len([track for track in yolo_tracker.tracks if track]))\n",
    "        yolo_memory_usages.append(memory_usage)\n",
    "        yolo_object_counts.append(len(yolo_detections))\n",
    "\n",
    "        # 성능 정보 표시\n",
    "        yolo_fps_text = f\"YOLO FPS: {1/frame_time:.1f}\"\n",
    "        yolo_count_text = f\"YOLO Tracks: {len([track for track in yolo_tracker.tracks if track])}\"\n",
    "        yolo_memory_text = f\"Memory: {memory_usage:.1f} MB\"\n",
    "        yolo_objects_text = f\"Objects: {len(yolo_detections)}\"\n",
    "        yolo_detection_time_text = f\"Det: {detection_time*1000:.1f}ms\"\n",
    "        yolo_tracking_time_text = f\"Track: {tracking_time*1000:.1f}ms\"\n",
    "\n",
    "        cv2.putText(yolo_vis_frame, yolo_fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(yolo_vis_frame, yolo_count_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(yolo_vis_frame, yolo_memory_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(yolo_vis_frame, yolo_objects_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(yolo_vis_frame, yolo_detection_time_text, (width-200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(yolo_vis_frame, yolo_tracking_time_text, (width-200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # 결과 영상 저장\n",
    "        yolo_output.write(yolo_vis_frame)\n",
    "\n",
    "    # 프레임 처리 시간 측정 종료\n",
    "    frames_processing_time = time.time() - frames_processing_start_time\n",
    "\n",
    "    # 전체 파이프라인 시간 측정 종료\n",
    "    total_time = time.time() - total_start_time\n",
    "\n",
    "    # 자원 해제\n",
    "    cap.release()\n",
    "    yolo_output.release()\n",
    "\n",
    "    # 방향 정확도 계산\n",
    "    yolo_direction_accuracy = yolo_tracker.get_direction_accuracy()\n",
    "\n",
    "    # 그리드 클러스터링 결과\n",
    "    yolo_valid_tracks, yolo_labels = yolo_tracker.cluster_tracks_by_grid()\n",
    "\n",
    "    # 결과 요약\n",
    "    results = {\n",
    "        # 단계별 시간 측정 결과\n",
    "        'avg_detection_time': np.mean(yolo_detection_times),\n",
    "        'avg_detection_fps': 1 / np.mean(yolo_detection_times),\n",
    "        'avg_tracking_time': np.mean(yolo_tracking_times),\n",
    "        'avg_tracking_fps': 1 / np.mean(yolo_tracking_times),\n",
    "        'avg_frame_processing_time': np.mean(yolo_processing_times),\n",
    "        'avg_frame_fps': 1 / np.mean(yolo_processing_times),\n",
    "\n",
    "        # 전체 파이프라인 시간 측정 결과\n",
    "        'model_setup_time': model_setup_time,\n",
    "        'frames_processing_time': frames_processing_time,\n",
    "        'total_pipeline_time': total_time,\n",
    "        'total_pipeline_fps': processed_frames / total_time,\n",
    "\n",
    "        # 기타 측정 결과\n",
    "        'avg_detections': np.mean(yolo_detection_counts),\n",
    "        'avg_objects': np.mean(yolo_object_counts),\n",
    "        'avg_memory_usage': np.mean(yolo_memory_usages),\n",
    "        'direction_accuracy': yolo_direction_accuracy,\n",
    "        'valid_tracks_count': len(yolo_valid_tracks),\n",
    "        'cluster_count': len(set(yolo_labels)) if yolo_labels else 0,\n",
    "        'processed_frames': processed_frames\n",
    "    }\n",
    "\n",
    "    # 메모리 정리\n",
    "    del yolo_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6N4ErXH_6zA"
   },
   "source": [
    "## 결과 시각화 및 분석 함수 (개선된 버전)\n",
    "\n",
    "두 방식의 트래킹 결과를 시각화하고 분석하는 함수를 정의합니다.\n",
    "개선된 성능 측정 결과를 더 자세히 비교할 수 있도록 수정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIddFWk__6zA"
   },
   "source": [
    "def visualize_results(seg_results, yolo_results):\n",
    "    \"\"\"두 방식의 결과 시각화 및 비교 (개선된 버전)\"\"\"\n",
    "    # 1. 기본 성능 지표 비교\n",
    "    metrics = ['프레임 처리 시간 (초)', '프레임 FPS', '전체 파이프라인 시간 (초)', '전체 파이프라인 FPS', '메모리 (MB)']\n",
    "    seg_values = [\n",
    "        seg_results['avg_frame_processing_time'],\n",
    "        seg_results['avg_frame_fps'],\n",
    "        seg_results['total_pipeline_time'],\n",
    "        seg_results['total_pipeline_fps'],\n",
    "        seg_results['avg_memory_usage']\n",
    "    ]\n",
    "    yolo_values = [\n",
    "        yolo_results['avg_frame_processing_time'],\n",
    "        yolo_results['avg_frame_fps'],\n",
    "        yolo_results['total_pipeline_time'],\n",
    "        yolo_results['total_pipeline_fps'],\n",
    "        yolo_results['avg_memory_usage']\n",
    "    ]\n",
    "\n",
    "    # 성능 지표 표 출력\n",
    "    print(\"===== 성능 지표 비교 =====\")\n",
    "    print(f\"{'지표':<25} {'GridClustering+Seg':>20} {'순정 YOLO':>15} {'차이(%)':>15}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        seg_val = seg_values[i]\n",
    "        yolo_val = yolo_values[i]\n",
    "\n",
    "        if yolo_val != 0:\n",
    "            diff_pct = ((seg_val - yolo_val) / yolo_val) * 100\n",
    "        else:\n",
    "            diff_pct = 0\n",
    "\n",
    "        print(f\"{metric:<25} {seg_val:>20.4f} {yolo_val:>15.4f} {diff_pct:>15.2f}%\")\n",
    "\n",
    "    # 2. 단계별 시간 비교\n",
    "    print(\"\\n===== 단계별 시간 비교 (초) =====\")\n",
    "    print(f\"{'단계':<25} {'GridClustering+Seg':>20} {'순정 YOLO':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(f\"{'모델 설정 시간':<25} {seg_results['model_setup_time']:>20.4f} {yolo_results['model_setup_time']:>15.4f}\")\n",
    "\n",
    "    if 'roi_setup_time' in seg_results:\n",
    "        print(f\"{'ROI 설정 시간':<25} {seg_results['roi_setup_time']:>20.4f} {'N/A':>15}\")\n",
    "\n",
    "    if 'avg_detection_time' in yolo_results:\n",
    "        print(f\"{'객체 검출 시간 (평균)':<25} {'N/A':>20} {yolo_results['avg_detection_time']:>15.4f}\")\n",
    "\n",
    "    if 'avg_tracking_time' in yolo_results:\n",
    "        print(f\"{'트래킹 시간 (평균)':<25} {'N/A':>20} {yolo_results['avg_tracking_time']:>15.4f}\")\n",
    "\n",
    "    print(f\"{'프레임 처리 시간 (평균)':<25} {seg_results['avg_frame_processing_time']:>20.4f} {yolo_results['avg_frame_processing_time']:>15.4f}\")\n",
    "    print(f\"{'전체 프레임 처리 시간':<25} {seg_results['frames_processing_time']:>20.4f} {yolo_results['frames_processing_time']:>15.4f}\")\n",
    "    print(f\"{'전체 파이프라인 시간':<25} {seg_results['total_pipeline_time']:>20.4f} {yolo_results['total_pipeline_time']:>15.4f}\")\n",
    "\n",
    "    # 3. 추가 지표 비교\n",
    "    print(\"\\n===== 추가 지표 비교 =====\")\n",
    "    print(f\"{'지표':<25} {'GridClustering+Seg':>20} {'순정 YOLO':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(f\"{'처리된 프레임 수':<25} {seg_results['processed_frames']:>20} {yolo_results['processed_frames']:>15}\")\n",
    "    print(f\"{'트랙 수 (평균)':<25} {seg_results['avg_detections']:>20.2f} {yolo_results['avg_detections']:>15.2f}\")\n",
    "\n",
    "    if 'avg_clusters' in seg_results:\n",
    "        print(f\"{'클러스터 수 (평균)':<25} {seg_results['avg_clusters']:>20.2f} {'N/A':>15}\")\n",
    "\n",
    "    if 'avg_objects' in yolo_results:\n",
    "        print(f\"{'검출된 객체 수 (평균)':<25} {'N/A':>20} {yolo_results['avg_objects']:>15.2f}\")\n",
    "\n",
    "    print(f\"{'유효 트랙 수':<25} {seg_results['valid_tracks_count']:>20} {yolo_results['valid_tracks_count']:>15}\")\n",
    "    print(f\"{'클러스터 수':<25} {seg_results['cluster_count']:>20} {yolo_results['cluster_count']:>15}\")\n",
    "\n",
    "    # 4. 성능 지표 시각화\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # 4.1 처리 시간 비교\n",
    "    plt.subplot(2, 2, 1)\n",
    "    time_metrics = ['프레임 처리', '전체 파이프라인']\n",
    "    seg_time_values = [seg_results['avg_frame_processing_time'], seg_results['total_pipeline_time']]\n",
    "    yolo_time_values = [yolo_results['avg_frame_processing_time'], yolo_results['total_pipeline_time']]\n",
    "\n",
    "    x = np.arange(len(time_metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, seg_time_values, width, label='GridClustering+Seg', color='skyblue')\n",
    "    plt.bar(x + width/2, yolo_time_values, width, label='순정 YOLO', color='salmon')\n",
    "\n",
    "    plt.xlabel('측정 단위')\n",
    "    plt.ylabel('시간 (초)')\n",
    "    plt.title('처리 시간 비교')\n",
    "    plt.xticks(x, time_metrics)\n",
    "    plt.legend()\n",
    "\n",
    "    # 막대 위에 값 표시\n",
    "    for i, v in enumerate(seg_time_values):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    for i, v in enumerate(yolo_time_values):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # 4.2 FPS 비교\n",
    "    plt.subplot(2, 2, 2)\n",
    "    fps_metrics = ['프레임 FPS', '전체 파이프라인 FPS']\n",
    "    seg_fps_values = [seg_results['avg_frame_fps'], seg_results['total_pipeline_fps']]\n",
    "    yolo_fps_values = [yolo_results['avg_frame_fps'], yolo_results['total_pipeline_fps']]\n",
    "\n",
    "    plt.bar(x - width/2, seg_fps_values, width, label='GridClustering+Seg', color='skyblue')\n",
    "    plt.bar(x + width/2, yolo_fps_values, width, label='순정 YOLO', color='salmon')\n",
    "\n",
    "    plt.xlabel('측정 단위')\n",
    "    plt.ylabel('FPS')\n",
    "    plt.title('FPS 비교')\n",
    "    plt.xticks(x, fps_metrics)\n",
    "    plt.legend()\n",
    "\n",
    "    # 막대 위에 값 표시\n",
    "    for i, v in enumerate(seg_fps_values):\n",
    "        plt.text(i - width/2, v + 0.5, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    for i, v in enumerate(yolo_fps_values):\n",
    "        plt.text(i + width/2, v + 0.5, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # 4.3 YOLO 단계별 시간 비교\n",
    "    if 'avg_detection_time' in yolo_results and 'avg_tracking_time' in yolo_results:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        yolo_steps = ['객체 검출', '트래킹', '전체 프레임 처리']\n",
    "        yolo_step_times = [\n",
    "            yolo_results['avg_detection_time'],\n",
    "            yolo_results['avg_tracking_time'],\n",
    "            yolo_results['avg_frame_processing_time']\n",
    "        ]\n",
    "\n",
    "        plt.bar(range(len(yolo_steps)), yolo_step_times, color='salmon')\n",
    "        plt.xlabel('처리 단계')\n",
    "        plt.ylabel('시간 (초)')\n",
    "        plt.title('YOLO 단계별 처리 시간')\n",
    "        plt.xticks(range(len(yolo_steps)), yolo_steps)\n",
    "\n",
    "        # 막대 위에 값 표시\n",
    "        for i, v in enumerate(yolo_step_times):\n",
    "            plt.text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # 4.4 메모리 사용량 비교\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(['GridClustering+Seg', '순정 YOLO'],\n",
    "            [seg_results['avg_memory_usage'], yolo_results['avg_memory_usage']],\n",
    "            color=['skyblue', 'salmon'])\n",
    "    plt.xlabel('방식')\n",
    "    plt.ylabel('메모리 사용량 (MB)')\n",
    "    plt.title('메모리 사용량 비교')\n",
    "\n",
    "    # 막대 위에 값 표시\n",
    "    plt.text(0, seg_results['avg_memory_usage'] + 10, f\"{seg_results['avg_memory_usage']:.2f}\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "    plt.text(1, yolo_results['avg_memory_usage'] + 10, f\"{yolo_results['avg_memory_usage']:.2f}\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_comparison_detailed.png')\n",
    "    plt.show()\n",
    "\n",
    "    # 5. 방향 정확도 비교\n",
    "    seg_direction = seg_results['direction_accuracy']\n",
    "    yolo_direction = yolo_results['direction_accuracy']\n",
    "\n",
    "    # 방향 정확도 CSV 저장\n",
    "    seg_df = pd.DataFrame.from_dict(seg_direction, orient='index')\n",
    "    seg_df.index.name = 'track_id'\n",
    "    seg_df.reset_index(inplace=True)\n",
    "    seg_df.to_csv('segmentation_direction_accuracy.csv', index=False)\n",
    "\n",
    "    yolo_df = pd.DataFrame.from_dict(yolo_direction, orient='index')\n",
    "    yolo_df.index.name = 'track_id'\n",
    "    yolo_df.reset_index(inplace=True)\n",
    "    yolo_df.to_csv('yolo_direction_accuracy.csv', index=False)\n",
    "\n",
    "    print(\"\\n===== 방향 정확도 비교 =====\")\n",
    "    print(f\"GridClustering+Seg 트랙 수: {len(seg_direction)}\")\n",
    "    print(f\"순정 YOLO 트랙 수: {len(yolo_direction)}\")\n",
    "\n",
    "    # 방향 정확도 평균 계산\n",
    "    seg_accuracy_avg = np.mean([data['direction_accuracy'] for data in seg_direction.values()]) if seg_direction else 0\n",
    "    yolo_accuracy_avg = np.mean([data['direction_accuracy'] for data in yolo_direction.values()]) if yolo_direction else 0\n",
    "\n",
    "    print(f\"GridClustering+Seg 평균 방향 정확도: {seg_accuracy_avg:.2f}%\")\n",
    "    print(f\"순정 YOLO 평균 방향 정확도: {yolo_accuracy_avg:.2f}%\")\n",
    "\n",
    "    # 방향 분포 시각화\n",
    "    def analyze_direction_distribution(direction_data):\n",
    "        direction_counts = {\"N\": 0, \"NE\": 0, \"E\": 0, \"SE\": 0, \"S\": 0, \"SW\": 0, \"W\": 0, \"NW\": 0}\n",
    "\n",
    "        for track_id, data in direction_data.items():\n",
    "            dominant_dir = data.get('dominant_direction')\n",
    "            if dominant_dir in direction_counts:\n",
    "                direction_counts[dominant_dir] += 1\n",
    "\n",
    "        return direction_counts\n",
    "\n",
    "    seg_dir_dist = analyze_direction_distribution(seg_direction)\n",
    "    yolo_dir_dist = analyze_direction_distribution(yolo_direction)\n",
    "\n",
    "    # 방향 분포 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    directions = list(seg_dir_dist.keys())\n",
    "    seg_counts = list(seg_dir_dist.values())\n",
    "    yolo_counts = list(yolo_dir_dist.values())\n",
    "\n",
    "    x = np.arange(len(directions))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, seg_counts, width, label='GridClustering+Seg', color='skyblue')\n",
    "    plt.bar(x + width/2, yolo_counts, width, label='순정 YOLO', color='salmon')\n",
    "\n",
    "    plt.xlabel('방향')\n",
    "    plt.ylabel('트랙 수')\n",
    "    plt.title('방향 분포 비교')\n",
    "    plt.xticks(x, directions)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('direction_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "    # 6. 결과 영상 비교\n",
    "    # 두 방식의 결과 영상을 나란히 비교하는 영상 생성\n",
    "    seg_video = cv2.VideoCapture('segmentation_tracking.mp4')\n",
    "    yolo_video = cv2.VideoCapture('yolo_tracking.mp4')\n",
    "\n",
    "    if not seg_video.isOpened() or not yolo_video.isOpened():\n",
    "        print(\"결과 영상을 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    width = int(seg_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(seg_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = seg_video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    comparison_output = cv2.VideoWriter('comparison.mp4', fourcc, fps, (width*2, height))\n",
    "\n",
    "    while True:\n",
    "        ret1, seg_frame = seg_video.read()\n",
    "        ret2, yolo_frame = yolo_video.read()\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "\n",
    "        # 비교 영상 생성\n",
    "        comparison_frame = np.zeros((height, width*2, 3), dtype=np.uint8)\n",
    "        comparison_frame[:, :width] = seg_frame\n",
    "        comparison_frame[:, width:] = yolo_frame\n",
    "\n",
    "        # 구분선 그리기\n",
    "        cv2.line(comparison_frame, (width, 0), (width, height), (255, 255, 255), 2)\n",
    "\n",
    "        # 제목 표시\n",
    "        cv2.putText(comparison_frame, \"GridClustering + Segmentation\", (width//4, 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        cv2.putText(comparison_frame, \"순정 YOLO\", (width + width//4, 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "        comparison_output.write(comparison_frame)\n",
    "\n",
    "    seg_video.release()\n",
    "    yolo_video.release()\n",
    "    comparison_output.release()\n",
    "    print(\"비교 영상 생성 완료: comparison.mp4\")\n",
    "\n",
    "    # 7. 결과 요약 저장\n",
    "    summary = {\n",
    "        'segmentation': seg_results,\n",
    "        'yolo': yolo_results\n",
    "    }\n",
    "\n",
    "    # JSON으로 저장\n",
    "    import json\n",
    "    with open('performance_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    print(\"\\n성능 측정 결과가 performance_summary.json 파일로 저장되었습니다.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs_7zUgm_6y9"
   },
   "source": [
    "## 메인 실행 코드\n",
    "\n",
    "두 방식을 순차적으로 실행하고 결과를 비교합니다.\n",
    "개선된 성능 측정 방식을 사용하여 더 정확한 비교가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ey91Aqn_6y9"
   },
   "source": [
    "def main(video_path, max_frames=100):\n",
    "    \"\"\"메인 실행 함수 (개선된 성능 측정)\"\"\"\n",
    "    print(\"===== Semantic Segmentation 기반 트래킹 실행 =====\")\n",
    "    seg_results = run_segmentation_tracking(video_path, max_frames)\n",
    "    \n",
    "    # 메모리 정리\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n===== YOLO 기반 트래킹 실행 =====\")\n",
    "    yolo_results = run_yolo_tracking(video_path, max_frames)\n",
    "    \n",
    "    print(\"\\n===== 결과 비교 및 시각화 =====\")\n",
    "    visualize_results(seg_results, yolo_results)\n",
    "    \n",
    "    return seg_results, yolo_results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs_7zUgm_6y9"
   },
   "source": [
    "## 실행 방법\n",
    "\n",
    "아래 코드를 실행하여 두 방식의 성능을 비교할 수 있습니다.\n",
    "\n",
    "```python\n",
    "# 비디오 파일 경로 설정\n",
    "video_path = 'sample.mp4'\n",
    "\n",
    "# 처리할 최대 프레임 수 설정 (메모리 관리를 위해 조정 가능)\n",
    "# None 또는 'none'으로 설정하면 전체 프레임 처리\n",
    "max_frames = 900  # 또는 None 또는 'none'\n",
    "\n",
    "# 두 방식 순차적으로 실행 및 결과 비교\n",
    "seg_results, yolo_results = main(video_path, max_frames)\n",
    "```\n",
    "\n",
    "참고: 프레임 수를 조정하여 메모리 사용량을 관리할 수 있습니다. 대회 제출 시에는 900 프레임 정도로 설정하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ey91Aqn_6y9"
   },
   "source": [
    "# 실행 예시\n",
    "# 비디오 파일 경로 설정\n",
    "video_path = 'sample.mp4'\n",
    "\n",
    "# 처리할 최대 프레임 수 설정 (메모리 관리를 위해 조정 가능)\n",
    "# None 또는 'none'으로 설정하면 전체 프레임 처리\n",
    "max_frames = 900  # 또는 None 또는 'none'\n",
    "\n",
    "# 두 방식 순차적으로 실행 및 결과 비교\n",
    "# seg_results, yolo_results = main(video_path, max_frames)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
